{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d352cd-0457-466b-afd4-4b8f3a93441c",
   "metadata": {},
   "source": [
    "# Sequence masking with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e9cd66-e2d3-4444-936c-1d4a925b7308",
   "metadata": {},
   "source": [
    "Resources:\n",
    "- Masking:\n",
    "    - [Difference between `src_mask` and `src_key_padding_mask` in PyTorch Transformer layers (from StackOverflow)](https://stackoverflow.com/questions/62170439/difference-between-src-mask-and-src-key-padding-mask)\n",
    "    - [UvA (University of Amsterdam) DL tutorial](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)\n",
    "    - [Judit √Åcs' blog post](https://juditacs.github.io/2018/12/27/masked-attention.html) (but watch out: **the attention matrix is not square!**)\n",
    "- Masked language modeling:\n",
    "    - [Kaggle notebook](https://www.kaggle.com/code/mojammel/masked-language-model-with-pytorch-transformer) (very similar to the PyTorch tutorial below)\n",
    "    - [MLM with BERT blog post](https://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c)\n",
    "    - [PyTorch transformer tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n",
    "    - [Tutorial with TensorFlow](https://keras.io/examples/nlp/masked_language_modeling/#create-bert-model-pretraining-model-for-masked-language-modeling) (this is actually a good reference, with tensor shapes etc.)\n",
    "    - [Tutorial with PyTorch](https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08e824b3-b56e-44bc-9526-c2783ab57d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.append('../../modules/')\n",
    "\n",
    "from models import TransformerClassifier, FFNN\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4633f1af-489a-4ebd-9629-c831ba2bb80d",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85becff7-d2a7-4e09-9a43-a5edc4f369fb",
   "metadata": {},
   "source": [
    "Generate a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "384a18d5-80da-4d90-af0e-be65054e2153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocabulary size.\n",
    "q = 8\n",
    "\n",
    "vocab = torch.arange(q).to(dtype=torch.int64)\n",
    "mask_idx = vocab.max() + 1\n",
    "\n",
    "# Enalarge the vocabulary with the special tokens.\n",
    "vocab = torch.hstack([vocab, torch.Tensor(mask_idx).to(dtype=torch.int64)])\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dda572-0edb-4477-b2a3-afd15c4c24dd",
   "metadata": {},
   "source": [
    "Generate sequences of tokens.\n",
    "\n",
    "**Note:** in the case considered all sequences have the same length and therefore no padding is (ever) needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3d287ab-557c-4871-9321-4d685c4e1ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7, 1, 6,  ..., 1, 5, 5],\n",
       "        [4, 7, 0,  ..., 7, 7, 4],\n",
       "        [1, 0, 2,  ..., 5, 5, 3],\n",
       "        ...,\n",
       "        [1, 4, 3,  ..., 4, 4, 4],\n",
       "        [6, 2, 3,  ..., 3, 5, 3],\n",
       "        [1, 1, 0,  ..., 1, 3, 3]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tree depth.\n",
    "l = 8\n",
    "\n",
    "batch_size = 64\n",
    "seq_len = 2 ** l\n",
    "\n",
    "sequences = torch.randint(q, (batch_size, seq_len))\n",
    "\n",
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e81a39-dab6-4e54-8254-4370a04bd57e",
   "metadata": {},
   "source": [
    "## Building attention masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbf0836-fc11-4fe7-9ae2-3ae130f09a0d",
   "metadata": {},
   "source": [
    "Given the sequences, generate trainable input embeddings for them (just the semantic part, we're skipping the positional encoding here as it's not essential)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c52ad3b-0c25-45fa-a49d-f77e6128a4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 128])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_dim = 128\n",
    "\n",
    "embedding_layer = torch.nn.Embedding(\n",
    "    num_embeddings=vocab.shape[0],\n",
    "    embedding_dim=hidden_dim\n",
    ")\n",
    "\n",
    "input_sequence_embeddings = embedding_layer(sequences)\n",
    "\n",
    "input_sequence_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da40c2f-c5f2-4159-8924-19085176008e",
   "metadata": {},
   "source": [
    "Instantiate a transformer Encoder model.\n",
    "\n",
    "**Note:** by convention, we stick with having the batch dimension as the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86de4d21-a971-44bb-9b2a-8333da8b72d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moscato/miniconda3/envs/tree-language/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 128])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A single encoder layer to be used in the full stack.\n",
    "encoder_layer = torch.nn.TransformerEncoderLayer(\n",
    "    d_model=hidden_dim,\n",
    "    nhead=1,\n",
    "    dim_feedforward=2048,\n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "# Stack of encoder layers.\n",
    "transformer_encoder = torch.nn.TransformerEncoder(\n",
    "    encoder_layer,\n",
    "    num_layers=1\n",
    ")\n",
    "\n",
    "encoder_output = transformer_encoder(input_sequence_embeddings)\n",
    "\n",
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d525b3-2332-403d-a1c9-6ea6d354c7b7",
   "metadata": {},
   "source": [
    "Generate masked sequences from the original one to perform masked language modeling (MLM): every token in every sequence is converted to the masked one (set conventionally) with some probability.\n",
    "\n",
    "**Mask:** we pass the mask as the encoder's `src_key_padding_mask` option, which means it should have shape `(batch_size, seq_len)` and (if of boolean type) contain `False` when no masking is needed and `True` when it is. In practice, it's obtained by simply checking the masked sequences against the padding value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5d12c93-79bb-44a1-86f6-d93c526e7076",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_sequences = sequences.clone()\n",
    "\n",
    "masking_rate = 0.1\n",
    "\n",
    "for i in range(sequences.shape[0]):\n",
    "    for j in range(sequences.shape[1]):\n",
    "        if torch.rand(1) < masking_rate:\n",
    "            masked_sequences[i, j] = mask_idx\n",
    "\n",
    "mask = (masked_sequences == mask_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0689478-3fec-4104-ba36-3b6513e2338b",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "- Should we pass the masked sequences or the original ones (always along with the mask) to the encoder?\n",
    "\n",
    "Answer: we should pass the masked sequences to the encoder, and then use the decoder to generate logits for every token in every sequence and compare this with the ground truth (non-masked sequences) via the cross-entropy loss.\n",
    "\n",
    "What to do with the **masked embeddings**?\n",
    " \n",
    " - If we pass the masked sequences, we should have input embeddings be created for the `<mask>` token, so it should be explicitly modeled. Could we indicate it as a padding token as we instantiate the `Embedding` layer for the input embeddings?\n",
    " \n",
    "A: the `<mask>` token should be explicitly modeled as part of the vocabulary. It shouldn't be among the possible **predicted** tokens though.\n",
    "\n",
    "- If we pass the original sequences we're leaving the original embeddings for the masked tokens, is the mask sufficient to tell the model to ignore them?\n",
    "\n",
    "A: masking is not sufficient. Indeed, we should pass the masked sequences to the encoder.\n",
    "\n",
    "- What about the gradient? Should we \"disconnect\" the masked token from the compute graph?\n",
    "\n",
    "A: still not clear. The attention mask should avoid connecting the embeddings of the masked tokens with the loss, but it's just a guess (and what about residual connections?).\n",
    "\n",
    "What about the **full model** (with a decoder as well)?\n",
    "\n",
    "- How do we tell which are the masked tokens to predict for in the sequences? The decoder won't know anything about the masking, so it'll have no way to distinguish between a masked token and a not masked one.\n",
    "\n",
    "A: it's true that the decoder won't know explicitly which are the masked tokens, but indirectly it will because we pass the masked sequences as input and because in the end we'll probably have to select ony the loss terms correspnding to the predictions for the masked tokens.\n",
    "\n",
    "- Should the decoder predict for one masked token at the time (how to select them? Mask them one by one?) or all masked tokens together (in which case, see the previous point)?\n",
    "\n",
    "A: the decoder should predict for all the tokens, masked and non-masked, so that for an input of shape `(batch_size, seq_len)` we get a final output of shape `(batch_size, seq_len, vocab_size)`, the last dimension corresponding to the logits over the vocabulary (restricted to the token that should be predicted, e.g. not `<mask>`). This means that we'll effectively predict for the entire sequence of tokens every time and computing the cross-entropy loss with the non-masked sequences we'll get a tensor of loss values of shape `(batch_size, seq_len)`. Now we can choose (not clear which is the right choice though!) whether to use compute the final loss as the mean over all the entries of this tensor or just over the ones corresponding to the masked tokens (by applying the mask over the tensor of loss values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5d067d2-0eea-46c0-9535-dfe4c5ad4403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2322,  0.0388,  1.6767,  ...,  0.0242,  1.3980,  1.6808],\n",
       "         [-0.8812,  0.0447,  0.0232,  ..., -0.4531,  0.2737,  0.0878],\n",
       "         [-1.5290,  0.0563,  0.0552,  ..., -0.2086,  0.0685, -0.2435],\n",
       "         ...,\n",
       "         [-0.4059, -0.8452, -0.8674,  ...,  0.4784,  0.0504,  1.4562],\n",
       "         [-0.3940,  0.8562, -0.2764,  ..., -0.2095,  0.3218,  0.1295],\n",
       "         [-0.0765,  0.9645, -0.4538,  ..., -0.2111,  0.3759,  0.1533]],\n",
       "\n",
       "        [[-0.4931, -1.5078, -0.5607,  ..., -1.6448,  1.0646,  0.1621],\n",
       "         [-0.2812,  0.0954,  1.4611,  ...,  0.0152,  1.1096,  1.7686],\n",
       "         [-0.7402, -1.2070,  0.0637,  ...,  0.4589, -0.2991,  1.0881],\n",
       "         ...,\n",
       "         [-0.4373,  0.1567,  1.2907,  ...,  0.0658,  1.3477,  1.7567],\n",
       "         [-0.1572,  0.3443,  1.5482,  ..., -0.1596,  1.2010,  1.7385],\n",
       "         [-0.3564, -1.5720, -0.8014,  ..., -1.6988,  0.9743,  0.0201]],\n",
       "\n",
       "        [[-0.6714, -0.9081, -0.8615,  ...,  0.3797, -0.1105,  1.4537],\n",
       "         [-1.2523, -0.8404,  0.0987,  ...,  0.4401, -0.3384,  0.9932],\n",
       "         [ 0.0319, -0.3194, -0.0873,  ...,  0.5533, -0.3225, -0.1043],\n",
       "         ...,\n",
       "         [-0.1880,  0.8980, -0.4621,  ..., -0.3977,  0.2578,  0.0538],\n",
       "         [-0.2452,  0.9323, -0.2870,  ..., -0.3256,  0.1777,  0.1647],\n",
       "         [-0.0712, -0.2422,  0.4202,  ...,  1.2026,  0.5604,  0.5035]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.5133, -0.9799, -0.8825,  ...,  0.2559, -0.0937,  1.4338],\n",
       "         [-0.5622, -1.5448, -0.6454,  ..., -1.3150,  1.0242, -0.0308],\n",
       "         [-1.4766, -0.0641,  0.2638,  ..., -0.1734,  0.2630,  0.0112],\n",
       "         ...,\n",
       "         [-0.4026, -1.5361, -0.5995,  ..., -1.7046,  1.1057,  0.1893],\n",
       "         [-0.3021, -1.4721, -0.5556,  ..., -1.7508,  1.0991,  0.1194],\n",
       "         [-0.1576, -1.6985, -0.7990,  ..., -1.5819,  0.8769,  0.0037]],\n",
       "\n",
       "        [[ 0.5292, -0.8147, -0.8395,  ..., -0.1797,  0.6801, -1.4072],\n",
       "         [ 0.0863, -0.4501, -0.2587,  ...,  0.4782, -0.1633,  0.0602],\n",
       "         [ 0.0272, -0.2667,  0.2878,  ...,  1.1549,  0.8248,  0.1093],\n",
       "         ...,\n",
       "         [-0.0040, -0.1708, -0.1577,  ...,  1.2447,  0.5459,  0.4861],\n",
       "         [-0.2695,  0.6714, -0.2805,  ..., -0.2754,  0.2124,  0.0452],\n",
       "         [-0.0261, -0.1928, -0.2343,  ...,  1.1063,  0.5296,  0.3449]],\n",
       "\n",
       "        [[-0.5611, -0.8955, -1.1586,  ...,  0.3124,  0.0359,  1.4451],\n",
       "         [-0.5533, -0.8546, -1.0713,  ...,  0.3636, -0.1100,  1.5495],\n",
       "         [-1.1518, -1.0519, -0.0483,  ...,  0.3549, -0.2691,  1.1234],\n",
       "         ...,\n",
       "         [-0.3342, -0.7817, -0.7566,  ...,  0.3944, -0.1522,  1.5586],\n",
       "         [-0.0057, -0.2639, -0.1424,  ...,  1.1056,  0.6077,  0.4794],\n",
       "         [-1.4084, -0.1754,  0.2569,  ..., -0.4765,  0.4565,  0.0178]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output_masked = transformer_encoder(\n",
    "    embedding_layer(masked_sequences),\n",
    "    src_key_padding_mask=mask\n",
    ")\n",
    "\n",
    "encoder_output_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb163e02-08c8-4643-a944-b4409829f5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 128])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output_masked.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00701d0-1bd0-4e1a-96aa-6c13a2feb2ff",
   "metadata": {},
   "source": [
    "The final output layer maps each token in each sequence to a set of logits (or probabilities) over the \"proper\" vocabulary (excluding special tokens), so it has tensors of shape `(batch_size, seq_len, hidden_dim)` as input and outputs tensors of shape `(batch_size, seq_len, vocab_size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bc792ab-f465-4d8c-a722-8d31381bf3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 256, 8]),\n",
       " torch.Size([64, 256, 8]),\n",
       " tensor([[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         ...,\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "        grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer = torch.nn.Linear(\n",
    "    in_features=hidden_dim,\n",
    "    out_features=q\n",
    ")\n",
    "\n",
    "output_logits = output_layer(encoder_output_masked)\n",
    "output_probs = torch.nn.Softmax(dim=-1)(output_logits)\n",
    "\n",
    "output_logits.shape, output_probs.shape, output_probs.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860b8c52-f1dc-48c2-86e1-88b259cc4631",
   "metadata": {},
   "source": [
    "Observation on the loss function (**should masking be considered at this stage?**):\n",
    "- We need the model (output layer) to output the predicted logits for each token in each sequence, i.e. a tensor of shape `(batch_size, seq_len, vocab_size)`, where the last dimension represents the logits over the vocabulary.\n",
    "- PyTorch's `CrossEntropyLoss` function accepts the logits and the true labels as the input, with the latter either as they are (class labels) or with one-hot encoding. In this case, if the predicted logits are put in the shape PyTorch expects (see point below), no one-hot encoding is needed for the targets.\n",
    "- Without any aggregation, we should have a value for the loss for each token in each sequence, which gives a tensor of loss values of shape `(batch_size, seq_len)`. PyTorch assumes that the predicted logits have shape `(batch_size, n_classes, [additional dims])`, so the last two dimensions of the output logits need to be switched.\n",
    "- **Guess:** probably the final loss should be computed only for the masked tokens, so we have to apply the mask to the loss tensor before aggregating the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31168f66-4498-42b1-be41-6fbab17dd4b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.7630, 1.8346, 2.2416,  ..., 2.3638, 2.4226, 2.6556],\n",
       "         [2.3062, 1.6842, 2.1129,  ..., 1.6795, 1.6789, 2.3856],\n",
       "         [2.4108, 1.9434, 1.3742,  ..., 2.6980, 2.6533, 2.3493],\n",
       "         ...,\n",
       "         [2.4751, 2.4196, 1.3885,  ..., 2.3911, 2.4065, 2.3443],\n",
       "         [1.9154, 1.4418, 2.4791,  ..., 2.2590, 2.6034, 2.3484],\n",
       "         [2.4772, 2.3990, 2.0787,  ..., 2.3824, 2.3456, 1.4215]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " torch.Size([64, 256]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(\n",
    "    reduction='none'  # Default: 'mean'.\n",
    ")\n",
    "\n",
    "loss_tensor = loss_fn(\n",
    "    torch.permute(output_logits, dims=(0, 2, 1)),\n",
    "    sequences\n",
    ")\n",
    "\n",
    "loss_tensor, loss_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df4d1a23-9207-41e1-899b-683b2cb9ffd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2145, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use masking if predicting only for the masked tokens,\n",
    "# drop the mask (!) to predict for the whole sequence.\n",
    "loss = loss_tensor[mask].mean()\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d961fb-3dbe-4ae3-b162-3f426ccdbc2a",
   "metadata": {},
   "source": [
    "## Building a model for MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c4cab39-f67a-4d76-95c3-af012a01b1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.6495e-01,  4.1420e-01,  7.3308e-01,  ...,  1.1685e-01,\n",
       "          -7.8136e-01, -7.0507e-02],\n",
       "         [-1.0830e+00,  5.0556e-01,  1.5749e+00,  ...,  5.5534e-01,\n",
       "          -7.4043e-01,  7.9937e-02],\n",
       "         [-3.5241e-01,  6.9179e-01,  1.8018e+00,  ...,  4.5706e-01,\n",
       "          -9.7880e-01, -4.2974e-01],\n",
       "         ...,\n",
       "         [-3.0115e-01,  2.1086e-01,  1.1576e+00,  ...,  3.0247e-01,\n",
       "           1.2638e-01, -6.9550e-01],\n",
       "         [-2.0102e-01,  7.2382e-01,  2.0122e-01,  ...,  4.9295e-02,\n",
       "           3.1154e-02, -4.5668e-01],\n",
       "         [-2.9357e-01,  5.9051e-01,  3.9578e-02,  ...,  5.6005e-02,\n",
       "           2.1187e-01,  2.4956e-01]],\n",
       "\n",
       "        [[-2.9578e-01, -1.5329e-01,  1.0964e+00,  ...,  6.1371e-01,\n",
       "          -1.2509e-01,  7.1609e-01],\n",
       "         [-6.5517e-02,  1.1782e-01,  1.0875e+00,  ...,  1.0212e-01,\n",
       "          -1.0336e+00,  3.1837e-02],\n",
       "         [-7.4655e-01,  8.2542e-01,  6.1313e-01,  ...,  6.9905e-01,\n",
       "          -5.5864e-01, -5.0904e-01],\n",
       "         ...,\n",
       "         [-5.2889e-01,  1.5720e-01,  8.4133e-02,  ..., -1.0856e-02,\n",
       "          -9.6491e-01, -1.3031e-01],\n",
       "         [-3.1754e-01, -1.6477e-01,  4.9025e-01,  ...,  7.4561e-02,\n",
       "          -1.0532e+00, -4.2352e-01],\n",
       "         [-1.1219e-02, -3.3880e-01,  4.3758e-01,  ...,  7.9055e-01,\n",
       "          -2.2860e-02,  6.2946e-01]],\n",
       "\n",
       "        [[-1.9655e-01,  9.9134e-01,  1.3935e+00,  ...,  3.2517e-01,\n",
       "          -6.1126e-01, -5.6034e-01],\n",
       "         [-2.1540e-01,  6.7645e-01,  4.1847e-01,  ...,  1.1234e+00,\n",
       "          -6.7938e-01, -3.1108e-01],\n",
       "         [-1.4655e-01,  1.3099e-01, -2.6717e-01,  ..., -4.3733e-01,\n",
       "          -7.8267e-01, -8.5584e-01],\n",
       "         ...,\n",
       "         [-3.7020e-01,  1.0795e-01, -3.0602e-04,  ...,  7.7537e-02,\n",
       "           2.3820e-01, -4.7255e-01],\n",
       "         [-3.7171e-01,  4.9939e-01,  1.9619e-01,  ...,  1.2740e-01,\n",
       "           4.9148e-01,  3.4399e-02],\n",
       "         [-4.7697e-02,  1.0724e+00,  4.1947e-01,  ...,  1.6833e-01,\n",
       "           2.1959e-01,  1.2524e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 5.5846e-02,  1.2776e+00,  1.5178e+00,  ...,  3.2755e-01,\n",
       "          -5.4184e-01, -5.6984e-01],\n",
       "         [ 1.1043e-01,  5.7403e-01,  7.7867e-01,  ...,  7.1613e-01,\n",
       "          -8.0324e-01,  5.5385e-01],\n",
       "         [-6.5316e-01,  7.5593e-01,  1.7249e+00,  ...,  4.9511e-01,\n",
       "          -8.0744e-01, -2.2323e-01],\n",
       "         ...,\n",
       "         [-1.6747e-01, -4.2241e-01,  7.5048e-01,  ...,  6.7414e-02,\n",
       "           1.5429e-01,  2.7672e-01],\n",
       "         [-2.1645e-01, -3.6598e-01,  7.5941e-01,  ...,  2.2830e-01,\n",
       "           1.1092e-01,  7.3479e-01],\n",
       "         [ 5.6584e-02,  4.6999e-02,  7.6368e-01,  ...,  4.4392e-01,\n",
       "          -7.3877e-01,  6.1310e-01]],\n",
       "\n",
       "        [[ 2.1622e-01, -4.8747e-01,  5.4724e-01,  ...,  7.5050e-01,\n",
       "          -4.8470e-01,  9.4471e-02],\n",
       "         [-7.1236e-01,  2.0888e-01, -2.5900e-02,  ..., -3.3778e-01,\n",
       "          -9.4872e-01, -5.7315e-01],\n",
       "         [-6.5957e-02,  1.6751e+00,  9.3029e-01,  ...,  6.4787e-02,\n",
       "          -4.4753e-01,  3.3715e-01],\n",
       "         ...,\n",
       "         [-3.3243e-01,  7.5594e-01,  7.7071e-01,  ...,  2.3247e-01,\n",
       "           2.8517e-01, -4.3762e-01],\n",
       "         [-3.4548e-01,  4.4824e-01,  3.4839e-01,  ...,  7.7311e-03,\n",
       "           2.1918e-01,  7.9373e-02],\n",
       "         [ 1.0327e-01,  1.4158e+00,  1.7878e-01,  ...,  2.5693e-01,\n",
       "           8.1013e-02, -2.4591e-01]],\n",
       "\n",
       "        [[-5.0945e-01,  7.8110e-01,  1.2666e+00,  ...,  4.9412e-01,\n",
       "          -1.8188e-01, -5.7649e-01],\n",
       "         [-4.6739e-01,  5.6759e-01,  1.3514e+00,  ...,  8.4690e-01,\n",
       "          -2.7407e-01, -4.4632e-01],\n",
       "         [-7.0337e-01,  1.0410e+00,  6.7413e-01,  ...,  8.9031e-01,\n",
       "          -7.4316e-01, -4.6366e-01],\n",
       "         ...,\n",
       "         [-4.5213e-01,  2.1364e-01,  1.1739e+00,  ...,  3.6472e-01,\n",
       "           1.4447e-02, -4.6706e-01],\n",
       "         [-5.2097e-01,  5.8396e-01,  4.7399e-01,  ...,  3.7389e-01,\n",
       "           3.5335e-01,  1.4404e-01],\n",
       "         [-6.1841e-01, -1.1270e-01,  1.0023e+00,  ...,  5.0525e-01,\n",
       "          -2.0185e-01, -8.4688e-02]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerClassifier(\n",
    "    seq_len=seq_len,\n",
    "    embedding_size=hidden_dim,\n",
    "    n_tranformer_layers=1,\n",
    "    n_heads=1,\n",
    "    vocab_size=vocab.shape[0],\n",
    "    n_special_tokens=1,\n",
    "    embedding_agg=None,\n",
    "    decoder_hidden_sizes=[],\n",
    "    decoder_activation='identity',\n",
    "    decoder_output_activation='identity'  # 'identity' --> Output logits\n",
    ")\n",
    "\n",
    "# Output shape: (batch_size, seq_len, vocab_size) (excluding\n",
    "# the special tokens from the vocabulary, which shouldn't be\n",
    "# predicted).\n",
    "model(masked_sequences, src_key_padding_mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "602e8654-4b7e-41f1-b7e1-58881a5eeee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2933, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "# Use masking if predicting only for the masked tokens,\n",
    "# drop the mask (!) to predict for the whole sequence.\n",
    "loss = loss_fn(\n",
    "    torch.permute(model(masked_sequences, src_key_padding_mask=mask), (0, 2, 1)),\n",
    "    sequences\n",
    ")[mask].mean()\n",
    "\n",
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
