{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d352cd-0457-466b-afd4-4b8f3a93441c",
   "metadata": {},
   "source": [
    "# Sequence masking with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e9cd66-e2d3-4444-936c-1d4a925b7308",
   "metadata": {},
   "source": [
    "Resources:\n",
    "- Masking:\n",
    "    - [Difference between `src_mask` and `src_key_padding_mask` in PyTorch Transformer layers (from StackOverflow)](https://stackoverflow.com/questions/62170439/difference-between-src-mask-and-src-key-padding-mask)\n",
    "    - [UvA (University of Amsterdam) DL tutorial](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)\n",
    "    - [Judit √Åcs' blog post](https://juditacs.github.io/2018/12/27/masked-attention.html) (but watch out: **the attention matrix is not square!**)\n",
    "- Masked language modeling:\n",
    "    - [Kaggle notebook](https://www.kaggle.com/code/mojammel/masked-language-model-with-pytorch-transformer)\n",
    "    - [MLM with BERT blog post](https://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c)\n",
    "    - [PyTorch transformer tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "08e824b3-b56e-44bc-9526-c2783ab57d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.append('../../modules/')\n",
    "\n",
    "from models import TransformerClassifier, FFNN\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4633f1af-489a-4ebd-9629-c831ba2bb80d",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85becff7-d2a7-4e09-9a43-a5edc4f369fb",
   "metadata": {},
   "source": [
    "Generate a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "384a18d5-80da-4d90-af0e-be65054e2153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocabulary size.\n",
    "q = 8\n",
    "\n",
    "vocab = torch.arange(q).to(dtype=torch.int64)\n",
    "mask_idx = -1\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dda572-0edb-4477-b2a3-afd15c4c24dd",
   "metadata": {},
   "source": [
    "Generate sequences of tokens.\n",
    "\n",
    "**Note:** in the case considered all sequences have the same length and therefore no padding is (ever) needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e3d287ab-557c-4871-9321-4d685c4e1ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 0, 2,  ..., 6, 6, 0],\n",
       "        [6, 2, 1,  ..., 3, 2, 5],\n",
       "        [7, 4, 3,  ..., 3, 5, 1],\n",
       "        ...,\n",
       "        [5, 4, 3,  ..., 7, 6, 5],\n",
       "        [4, 1, 6,  ..., 0, 2, 3],\n",
       "        [2, 1, 4,  ..., 3, 7, 5]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tree depth.\n",
    "l = 8\n",
    "\n",
    "batch_size = 64\n",
    "seq_len = 2 ** l\n",
    "\n",
    "sequences = torch.randint(q, (batch_size, seq_len))\n",
    "\n",
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e81a39-dab6-4e54-8254-4370a04bd57e",
   "metadata": {},
   "source": [
    "## Building attention masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbf0836-fc11-4fe7-9ae2-3ae130f09a0d",
   "metadata": {},
   "source": [
    "Given the sequences, generate trainable input embeddings for them (just the semantic part, we're skipping the positional encoding here as it's not essential)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4c52ad3b-0c25-45fa-a49d-f77e6128a4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 128])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_dim = 128\n",
    "\n",
    "embedding_layer = torch.nn.Embedding(\n",
    "    num_embeddings=vocab.shape[0],\n",
    "    embedding_dim=hidden_dim\n",
    ")\n",
    "\n",
    "input_sequence_embeddings = embedding_layer(sequences)\n",
    "\n",
    "input_sequence_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da40c2f-c5f2-4159-8924-19085176008e",
   "metadata": {},
   "source": [
    "Instantiate a transformer Encoder model.\n",
    "\n",
    "**Note:** by convention, we stick with having the batch dimension as the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "86de4d21-a971-44bb-9b2a-8333da8b72d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moscato/miniconda3/envs/tree-language/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 128])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A single encoder layer to be used in the full stack.\n",
    "encoder_layer = torch.nn.TransformerEncoderLayer(\n",
    "    d_model=hidden_dim,\n",
    "    nhead=1,\n",
    "    dim_feedforward=2048,\n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "# Stack of encoder layers.\n",
    "transformer_encoder = torch.nn.TransformerEncoder(\n",
    "    encoder_layer,\n",
    "    num_layers=1\n",
    ")\n",
    "\n",
    "encoder_output = transformer_encoder(input_sequence_embeddings)\n",
    "\n",
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d525b3-2332-403d-a1c9-6ea6d354c7b7",
   "metadata": {},
   "source": [
    "Generate masked sequences from the original one to perform masked language modeling (MLM): every token in every sequence is converted to the masked one (set conventionally) with some probability.\n",
    "\n",
    "**Mask:** we pass the mask as the encoder's `src_key_padding_mask` option, which means it should have shape `(batch_size, seq_len)` and (if of boolean type) contain `False` when no masking is needed and `True` when it is. In practice, it's obtained by simply checking the masked sequences against the padding value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b5d12c93-79bb-44a1-86f6-d93c526e7076",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_sequences = sequences.clone()\n",
    "\n",
    "masking_rate = 0.1\n",
    "\n",
    "for i in range(sequences.shape[0]):\n",
    "    for j in range(sequences.shape[1]):\n",
    "        if torch.rand(1) < masking_rate:\n",
    "            masked_sequences[i, j] = mask_idx\n",
    "\n",
    "mask = (masked_sequences == mask_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0689478-3fec-4104-ba36-3b6513e2338b",
   "metadata": {},
   "source": [
    "Questions:\n",
    "- Should we pass the masked sequences or the original ones (always along with the mask) to the encoder?\n",
    "- What to do with the masked embeddings?\n",
    "    - If we pass the masked sequences, we should have input embeddings be created for the `<mask>` token, so it should be explicitly modeled. Could we indicate it as a padding token as we instantiate the `Embedding` layer for the input embeddings?\n",
    "    - If we pass the original sequences we're leaving the original embeddings for the masked tokens, is the mask sufficient to tell the model to ignore them?\n",
    "    - What about the gradient? Should we \"disconnect\" the masked token from the compute graph?\n",
    "- Full model (with a decoder as well)\n",
    "    - How do we tell which are the masked tokens to predict for in the sequences? The decoder won't know anything about the masking, so it'll have no way to distinguish between a masked token and a not masked one.\n",
    "    - Should the decoder predict for one masked token at the time (how to select them? Mask them one by one?) or all masked tokens together (in which case, see the previous point)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b5d067d2-0eea-46c0-9535-dfe4c5ad4403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4672,  1.1940,  1.0537,  ..., -0.2877,  0.2319,  1.2596],\n",
       "         [-0.2028,  0.1728,  1.5650,  ..., -0.5773,  0.6646,  0.9247],\n",
       "         [ 0.1062,  1.3121,  1.1403,  ..., -0.2368,  0.2331,  1.2061],\n",
       "         ...,\n",
       "         [-0.1510,  0.3138,  0.1510,  ..., -1.4177,  0.7866, -0.8487],\n",
       "         [-0.2009,  0.4738,  0.1920,  ..., -1.2997,  0.8303, -0.8554],\n",
       "         [-0.2200,  0.2029,  0.9257,  ..., -0.6490,  0.3923,  0.8745]],\n",
       "\n",
       "        [[ 0.0445,  0.3207,  0.2282,  ..., -1.1129,  0.7127, -0.8959],\n",
       "         [ 0.1751,  1.4233,  1.2060,  ..., -0.4689,  0.2305,  1.2587],\n",
       "         [-0.6427, -1.7986, -1.6010,  ...,  1.7697,  0.8395,  0.0797],\n",
       "         ...,\n",
       "         [ 0.7727, -0.1183,  1.1160,  ..., -1.9133, -0.5322, -0.8142],\n",
       "         [ 0.2510,  1.4098,  1.1894,  ..., -0.2762,  0.1966,  1.1853],\n",
       "         [ 2.3494, -1.3264,  1.3115,  ...,  0.4348,  0.6838,  0.2742]],\n",
       "\n",
       "        [[-1.5283,  1.4894, -2.1376,  ...,  0.7907,  0.0856,  0.0318],\n",
       "         [-1.1355, -0.3074,  1.4480,  ...,  0.7797,  1.1222,  0.0182],\n",
       "         [ 0.8121, -0.1242,  1.1246,  ..., -1.8447, -0.6434, -0.9268],\n",
       "         ...,\n",
       "         [ 0.5723, -0.4469,  1.1551,  ..., -2.0064, -0.6342, -0.7106],\n",
       "         [ 2.2429, -1.3758,  1.3449,  ...,  0.4687,  0.5965,  0.4040],\n",
       "         [-0.6395, -1.6489, -1.6105,  ...,  1.7605,  0.7131,  0.2315]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.3075, -1.3664,  1.3471,  ...,  0.2276,  0.6868,  0.4444],\n",
       "         [-1.2806, -0.3674,  1.3496,  ...,  0.8889,  0.9077, -0.0122],\n",
       "         [ 0.8772, -0.3255,  0.8385,  ..., -1.9117, -0.7228, -0.8237],\n",
       "         ...,\n",
       "         [-1.3118,  1.5973, -1.9134,  ...,  1.0250,  0.1413, -0.3497],\n",
       "         [-0.2305,  0.3160,  0.3507,  ..., -1.2523,  0.7764, -0.9654],\n",
       "         [ 2.4713, -1.3674,  1.2609,  ...,  0.4539,  0.5751,  0.5228]],\n",
       "\n",
       "        [[-1.0674, -0.6862,  1.4557,  ...,  0.8799,  1.2081, -0.2496],\n",
       "         [-0.6599, -2.0005, -1.5020,  ...,  1.8899,  1.1137,  0.2850],\n",
       "         [-0.2019,  0.4818,  0.2527,  ..., -1.3526,  0.7093, -0.9617],\n",
       "         ...,\n",
       "         [-0.1870,  0.2392,  1.4386,  ..., -0.6476,  0.6265,  0.9721],\n",
       "         [ 0.1905,  1.4556,  1.1358,  ..., -0.4660,  0.1370,  1.2998],\n",
       "         [ 0.8878, -0.2485,  1.1738,  ..., -1.9928, -0.6103, -0.8245]],\n",
       "\n",
       "        [[ 0.2461,  1.4108,  1.2344,  ..., -0.3817,  0.0987,  1.2210],\n",
       "         [-0.7027, -1.8508, -1.7977,  ...,  1.7177,  0.9607,  0.2863],\n",
       "         [-1.2005, -0.7511,  1.4316,  ...,  0.9839,  1.1539,  0.0863],\n",
       "         ...,\n",
       "         [ 0.7833, -0.4626,  0.9368,  ..., -1.9105, -0.5923, -0.8258],\n",
       "         [-1.4647,  1.4531, -1.8870,  ...,  0.8851,  0.0159, -0.4995],\n",
       "         [ 2.3566, -1.3402,  1.0950,  ...,  0.4166,  0.4584,  0.4817]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output_masked = transformer_encoder(\n",
    "    embedding_layer(sequences),\n",
    "    src_key_padding_mask=mask\n",
    ")\n",
    "\n",
    "encoder_output_masked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d961fb-3dbe-4ae3-b162-3f426ccdbc2a",
   "metadata": {},
   "source": [
    "## Building a model for MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "2c4cab39-f67a-4d76-95c3-af012a01b1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0560, 0.0959, 0.2724, 0.1043, 0.0876, 0.1265, 0.0803, 0.1770],\n",
       "        [0.0289, 0.1510, 0.1804, 0.1897, 0.0853, 0.0797, 0.1354, 0.1497],\n",
       "        [0.0526, 0.2401, 0.1393, 0.0856, 0.2161, 0.0724, 0.0434, 0.1505],\n",
       "        [0.0303, 0.1611, 0.1556, 0.1782, 0.0878, 0.0785, 0.0614, 0.2470],\n",
       "        [0.0214, 0.1749, 0.2697, 0.1860, 0.0697, 0.1506, 0.0484, 0.0793],\n",
       "        [0.1414, 0.0967, 0.1740, 0.2270, 0.1583, 0.0766, 0.0446, 0.0814],\n",
       "        [0.0255, 0.1447, 0.2715, 0.1412, 0.1098, 0.1030, 0.0685, 0.1357],\n",
       "        [0.1256, 0.1061, 0.1431, 0.0518, 0.2128, 0.0948, 0.0517, 0.2140],\n",
       "        [0.0555, 0.0858, 0.2906, 0.1301, 0.0686, 0.1281, 0.1336, 0.1078],\n",
       "        [0.0290, 0.0998, 0.3629, 0.0666, 0.0675, 0.0793, 0.0261, 0.2688],\n",
       "        [0.0860, 0.0718, 0.1642, 0.1850, 0.0947, 0.1615, 0.0834, 0.1534],\n",
       "        [0.0266, 0.1494, 0.2083, 0.1140, 0.1752, 0.0811, 0.0701, 0.1754],\n",
       "        [0.0904, 0.1651, 0.1151, 0.3147, 0.0973, 0.0857, 0.0444, 0.0871],\n",
       "        [0.0477, 0.0898, 0.1403, 0.1311, 0.1396, 0.1779, 0.0923, 0.1813],\n",
       "        [0.0394, 0.2152, 0.3146, 0.1041, 0.1081, 0.1099, 0.0332, 0.0755],\n",
       "        [0.0267, 0.0513, 0.3997, 0.1674, 0.0808, 0.1181, 0.0428, 0.1132],\n",
       "        [0.0166, 0.2324, 0.1267, 0.1507, 0.0701, 0.1492, 0.0346, 0.2197],\n",
       "        [0.0453, 0.1065, 0.1987, 0.1853, 0.1058, 0.1191, 0.0376, 0.2017],\n",
       "        [0.0228, 0.2008, 0.2027, 0.1472, 0.2144, 0.0629, 0.0432, 0.1060],\n",
       "        [0.0656, 0.2280, 0.1447, 0.2372, 0.1248, 0.0615, 0.0747, 0.0635],\n",
       "        [0.0443, 0.1785, 0.0913, 0.2346, 0.0669, 0.1773, 0.0264, 0.1806],\n",
       "        [0.0855, 0.0768, 0.2587, 0.1128, 0.2111, 0.1693, 0.0247, 0.0612],\n",
       "        [0.1385, 0.0992, 0.1050, 0.2710, 0.0797, 0.0562, 0.0479, 0.2024],\n",
       "        [0.0351, 0.2304, 0.1024, 0.3100, 0.0770, 0.0440, 0.0659, 0.1352],\n",
       "        [0.0235, 0.1553, 0.2046, 0.1684, 0.1754, 0.1006, 0.0636, 0.1086],\n",
       "        [0.0624, 0.1161, 0.2570, 0.1177, 0.0719, 0.0700, 0.0582, 0.2467],\n",
       "        [0.0492, 0.1572, 0.1291, 0.1607, 0.1466, 0.0508, 0.0415, 0.2649],\n",
       "        [0.0279, 0.0389, 0.4015, 0.2363, 0.0752, 0.0677, 0.0296, 0.1228],\n",
       "        [0.0775, 0.1036, 0.1097, 0.3513, 0.1179, 0.0920, 0.0539, 0.0940],\n",
       "        [0.0582, 0.1466, 0.1312, 0.1532, 0.2754, 0.0818, 0.0503, 0.1031],\n",
       "        [0.1327, 0.0961, 0.2526, 0.1365, 0.0569, 0.1218, 0.0593, 0.1441],\n",
       "        [0.0553, 0.1601, 0.1661, 0.1525, 0.0607, 0.0732, 0.0988, 0.2334],\n",
       "        [0.0379, 0.1994, 0.1234, 0.1516, 0.1763, 0.1179, 0.0483, 0.1452],\n",
       "        [0.0541, 0.1421, 0.2493, 0.1576, 0.1078, 0.1035, 0.0615, 0.1242],\n",
       "        [0.0271, 0.1233, 0.2696, 0.1641, 0.1363, 0.1246, 0.0202, 0.1349],\n",
       "        [0.0733, 0.0616, 0.2355, 0.2116, 0.0881, 0.0992, 0.0908, 0.1398],\n",
       "        [0.0659, 0.1165, 0.3484, 0.2133, 0.0392, 0.0880, 0.0599, 0.0686],\n",
       "        [0.0187, 0.2101, 0.3117, 0.1990, 0.0453, 0.0415, 0.0401, 0.1335],\n",
       "        [0.0469, 0.2021, 0.2195, 0.0866, 0.1900, 0.0689, 0.0907, 0.0952],\n",
       "        [0.0439, 0.0633, 0.2990, 0.0815, 0.0667, 0.1922, 0.0757, 0.1778],\n",
       "        [0.0204, 0.1667, 0.1189, 0.3036, 0.0416, 0.0415, 0.0932, 0.2141],\n",
       "        [0.0834, 0.1394, 0.2090, 0.2552, 0.0521, 0.0775, 0.0828, 0.1006],\n",
       "        [0.1263, 0.1350, 0.1928, 0.0810, 0.0479, 0.1119, 0.0720, 0.2332],\n",
       "        [0.0477, 0.2019, 0.2569, 0.0598, 0.0866, 0.1169, 0.0390, 0.1913],\n",
       "        [0.0552, 0.1371, 0.1189, 0.1840, 0.1228, 0.1912, 0.0495, 0.1413],\n",
       "        [0.0979, 0.2997, 0.2639, 0.1093, 0.0493, 0.0885, 0.0228, 0.0685],\n",
       "        [0.0584, 0.0761, 0.1510, 0.1843, 0.1937, 0.1466, 0.0808, 0.1091],\n",
       "        [0.0236, 0.0464, 0.1926, 0.1112, 0.2605, 0.0776, 0.0683, 0.2197],\n",
       "        [0.0768, 0.3112, 0.1741, 0.1179, 0.0925, 0.0692, 0.0353, 0.1230],\n",
       "        [0.0829, 0.1866, 0.2316, 0.0948, 0.1196, 0.0783, 0.0909, 0.1152],\n",
       "        [0.0446, 0.2550, 0.1641, 0.0828, 0.0826, 0.1297, 0.0690, 0.1723],\n",
       "        [0.0398, 0.2113, 0.0730, 0.1091, 0.1096, 0.1200, 0.0512, 0.2860],\n",
       "        [0.0227, 0.1289, 0.0984, 0.1970, 0.1697, 0.1456, 0.1069, 0.1308],\n",
       "        [0.0311, 0.1331, 0.1133, 0.2263, 0.1031, 0.2087, 0.0272, 0.1572],\n",
       "        [0.0197, 0.1659, 0.1252, 0.3135, 0.0901, 0.0971, 0.0395, 0.1491],\n",
       "        [0.0316, 0.1095, 0.2457, 0.1393, 0.1686, 0.1052, 0.0391, 0.1610],\n",
       "        [0.0694, 0.1359, 0.2035, 0.2469, 0.0784, 0.0587, 0.0519, 0.1552],\n",
       "        [0.0176, 0.1870, 0.2251, 0.1997, 0.0631, 0.1551, 0.0531, 0.0993],\n",
       "        [0.0701, 0.1520, 0.1634, 0.1517, 0.0982, 0.1979, 0.0955, 0.0712],\n",
       "        [0.0299, 0.0980, 0.1469, 0.2743, 0.0785, 0.0820, 0.0971, 0.1932],\n",
       "        [0.0457, 0.1285, 0.2274, 0.1541, 0.1771, 0.1118, 0.0533, 0.1022],\n",
       "        [0.0245, 0.1956, 0.1428, 0.1538, 0.0600, 0.2337, 0.0549, 0.1347],\n",
       "        [0.0558, 0.1670, 0.2194, 0.1638, 0.0812, 0.1603, 0.0268, 0.1257],\n",
       "        [0.0536, 0.0973, 0.3115, 0.0830, 0.1268, 0.0883, 0.0563, 0.1833]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerClassifier(\n",
    "    seq_len=seq_len,\n",
    "    embedding_size=hidden_dim,\n",
    "    n_tranformer_layers=1,\n",
    "    n_heads=1,\n",
    "    n_classes=q,\n",
    "    embedding_agg='flatten',\n",
    "    decoder_hidden_sizes=[],\n",
    "    decoder_activation='identity',\n",
    "    decoder_output_activation='softmax'\n",
    ")\n",
    "\n",
    "model(sequences, src_key_padding_mask=mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
