{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98044490-e4ee-4dca-bb99-99f8151ff366",
   "metadata": {},
   "source": [
    "# Root inference with a neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67847692-f5c3-4217-969a-3147739a2b7b",
   "metadata": {},
   "source": [
    "__Obejctive:__ experiment with generating data and fitting a NN model to predict the root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b512400d-e637-454e-ab9e-04587e76f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "sys.path.append('../modules/')\n",
    "\n",
    "from logger import get_logger\n",
    "from tree_generation import calcrho, generate_trees, compute_rho_entropy\n",
    "from models import FFNN\n",
    "from training import training_step, EarlyStopper\n",
    "from model_evaluation import compute_accuracy, load_experiment_catalog, save_experiment_info\n",
    "from plotting import plot_training_history\n",
    "\n",
    "sys.path.append('../scripts/')\n",
    "\n",
    "from RootInference_current import get_M, get_leaves\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Available device:', device)\n",
    "\n",
    "logger = get_logger('nn_test')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c7a1f0-fd67-42c8-838f-ba2ae927f749",
   "metadata": {},
   "source": [
    "## Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15945b3e-71ad-4645-a612-f5360933c73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/'\n",
    "EXPERIMENT_CATALOG_PATH = '../data/experiment_catalog.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab27ea68-956b-4b02-baf9-55f8d30b68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_matrix = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d35998-2353-4eca-bb95-272c4f5fc873",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = np.logspace(-5,0,12)\n",
    "epsilons = np.insert(epsilons,0,0)\n",
    "\n",
    "sigmas = np.linspace(0,3,16)\n",
    "\n",
    "np.random.seed(14)\n",
    "\n",
    "n_samples = 8000\n",
    "k = 5\n",
    "q = 4\n",
    "matrix_type = 'mixed_index_sets'\n",
    "calcrho_kwargs = dict(\n",
    "    q=q,\n",
    "    sigma=sigmas[-1],\n",
    "    epsilon=epsilons[0]\n",
    ")\n",
    "\n",
    "rho = calcrho(matrix_type, **calcrho_kwargs)\n",
    "rho, trees, roots, leaves = generate_trees(rho, n_samples, k, q)\n",
    "\n",
    "rho_entropy = compute_rho_entropy(rho, q)\n",
    "\n",
    "roots, leaves, leaves.shape, rho_entropy, rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbf321e-f157-4bda-b057-2e359bcb18f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Save generated data.\n",
    "# dataset_name = 'exp_2'\n",
    "\n",
    "# dataset_dir = os.path.join(DATA_DIR, dataset_name)\n",
    "\n",
    "# if not os.path.exists(dataset_dir):\n",
    "#     os.makedirs(dataset_dir)\n",
    "\n",
    "#     logger.info(f'Created data directory: {dataset_dir}')\n",
    "\n",
    "# with open(os.path.join(dataset_dir, 'rho.npy'), 'wb') as f:\n",
    "#     np.save(f, rho)\n",
    "\n",
    "# with open(os.path.join(dataset_dir, 'roots.npy'), 'wb') as f:\n",
    "#     np.save(f, roots)\n",
    "\n",
    "# with open(os.path.join(dataset_dir, 'trees.npy'), 'wb') as f:\n",
    "#     np.save(f, trees)\n",
    "\n",
    "# with open(os.path.join(dataset_dir, 'leaves.npy'), 'wb') as f:\n",
    "#     np.save(f, leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89af28e1-9494-45d5-90bc-02b60ffbe046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # To load a saved dataset.\n",
    "# dataset_name = 'exp_1'\n",
    "\n",
    "# dataset_dir = os.path.join(DATA_DIR, dataset_name)\n",
    "\n",
    "# with open(os.path.join(dataset_dir, 'rho.npy'), 'rb') as f:\n",
    "#     rho = np.load(f)\n",
    "\n",
    "# with open(os.path.join(dataset_dir, 'roots.npy'), 'rb') as f:\n",
    "#     roots = np.load(f)\n",
    "\n",
    "# with open(os.path.join(dataset_dir, 'trees.npy'), 'rb') as f:\n",
    "#     trees = np.load(f)\n",
    "\n",
    "# with open(os.path.join(dataset_dir, 'leaves.npy'), 'rb') as f:\n",
    "#     leaves = np.load(f)\n",
    "\n",
    "# q = np.unique(roots).shape[0]\n",
    "\n",
    "# print(f'N roots: {q} | N leaves: {leaves.shape[1]} | N samples: {leaves.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04760952-2a71-455e-aeab-142b68ef3b5d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e424628-1701-4442-bf12-83f3bd3a2a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mask_input(leaves, prob, masked_symbols_bound=1):\n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#     for i in range(leaves.shape[0]):\n",
    "#         masked_symbols = 0\n",
    "        \n",
    "#         for j in range(leaves.shape[1]):\n",
    "#             if np.random.uniform() < prob:\n",
    "#                 leaves[i, j] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a94f9a-ea10-4615-8cd4-f3aa191948cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train-test split.\n",
    "# test_frac = .2\n",
    "\n",
    "# test_indices = np.random.choice(range(leaves.shape[0]), int(leaves.shape[0] * test_frac), replace=False)\n",
    "# train_indices = np.array(list(set(range(leaves.shape[0])) - set(test_indices)))\n",
    "\n",
    "# x_train = torch.nn.functional.one_hot(torch.from_numpy(leaves[train_indices, :]), num_classes=q).to(dtype=torch.float32).to(device=device)\n",
    "# y_train = torch.nn.functional.one_hot(torch.from_numpy(roots[train_indices]), num_classes=q).to(dtype=torch.float32).to(device=device)\n",
    "# x_test = torch.nn.functional.one_hot(torch.from_numpy(leaves[test_indices, :]), num_classes=q).to(dtype=torch.float32).to(device=device)\n",
    "# y_test = torch.nn.functional.one_hot(torch.from_numpy(roots[test_indices]), num_classes=q).to(dtype=torch.float32).to(device=device)\n",
    "\n",
    "x_train = torch.nn.functional.one_hot(torch.from_numpy(leaves), num_classes=q).to(dtype=torch.float32).to(device=device)\n",
    "y_train = torch.nn.functional.one_hot(torch.from_numpy(roots), num_classes=q).to(dtype=torch.float32).to(device=device)\n",
    "\n",
    "n_test_samples = 2000\n",
    "\n",
    "_, _, test_roots, test_leaves = generate_trees(rho, n_test_samples, k, q)\n",
    "\n",
    "x_test = torch.nn.functional.one_hot(torch.from_numpy(test_leaves), num_classes=q).to(dtype=torch.float32).to(device=device)\n",
    "y_test = torch.nn.functional.one_hot(torch.from_numpy(test_roots), num_classes=q).to(dtype=torch.float32).to(device=device)\n",
    "\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13435c0e-bc91-463c-8896-60189ca14daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = [leaves.shape[-1], 64, q]\n",
    "\n",
    "model = FFNN(\n",
    "    dims=dims,\n",
    "    activation='relu',\n",
    "    output_activation='softmax',\n",
    "    batch_normalization=False,\n",
    "    concatenate_last_dim=True\n",
    ").to(device=device)\n",
    "\n",
    "print(f'N params: {sum(p.numel() for p in model.parameters())}')\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "epoch_counter = 0\n",
    "\n",
    "training_history = {\n",
    "    'training_loss': [],\n",
    "    'val_loss': [],\n",
    "    'training_accuracy': [],\n",
    "    'val_accuracy': []\n",
    "}\n",
    "\n",
    "# Test.\n",
    "loss_fn(\n",
    "    model(x_train),\n",
    "    y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18629b39-ce9a-4b92-b45a-9765fe4cbe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters(),\n",
    "    lr=learning_rate\n",
    ")\n",
    "\n",
    "# early_stopper = EarlyStopper(\n",
    "#     patience=5,\n",
    "#     min_delta=0.\n",
    "# )\n",
    "early_stopper = None\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(x_train, y_train),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "n_epochs = 150\n",
    "\n",
    "# Training loop.\n",
    "for i in range(n_epochs):\n",
    "    epoch_counter += 1\n",
    "\n",
    "    training_loss_batches = []\n",
    "    training_accuracy_batches = []\n",
    "\n",
    "    for batch in training_loader:\n",
    "        training_batch, training_targets = batch\n",
    "    \n",
    "        training_loss_batch, _ = training_step(\n",
    "            (training_batch, training_targets),\n",
    "            model,\n",
    "            loss_fn,\n",
    "            optimizer,\n",
    "        )\n",
    "\n",
    "        training_loss_batches.append(training_loss_batch)\n",
    "\n",
    "        # Compute the training accuracy over the batch and append it to\n",
    "        # the corresponding list.\n",
    "        training_accuracy_batch = compute_accuracy(model(training_batch), training_targets)\n",
    "        training_accuracy_batches.append(training_accuracy_batch)\n",
    "\n",
    "    # Training loss and accuracy for one epoch is computed as the average\n",
    "    # training loss over the batches.\n",
    "    training_loss = torch.tensor(training_loss_batches).mean()\n",
    "    training_accuracy = torch.tensor(training_accuracy_batches).mean()\n",
    "\n",
    "    training_history['training_loss'].append(training_loss)\n",
    "    training_history['training_accuracy'].append(training_accuracy)\n",
    "\n",
    "    if x_test is not None:\n",
    "        with torch.no_grad():\n",
    "            val_loss = loss_fn(model(x_test), y_test)\n",
    "            val_accuracy = compute_accuracy(model(x_test), y_test)\n",
    "    else:\n",
    "        val_loss = None\n",
    "        val_accuracy = None\n",
    "\n",
    "    training_history['val_loss'].append(\n",
    "        val_loss if val_loss is not None else None\n",
    "    )\n",
    "\n",
    "    training_history['val_accuracy'].append(\n",
    "        val_accuracy if val_accuracy is not None else None\n",
    "    )\n",
    "\n",
    "    if (i < 50) or (i % 50 == 0):\n",
    "        logger.debug(\n",
    "            f'Epoch: {epoch_counter}'\n",
    "            f' | Training loss: {training_history[\"training_loss\"][-1]}'\n",
    "            f' | Validation loss: {training_history[\"val_loss\"][-1]}'\n",
    "        )\n",
    "\n",
    "    if (x_test is not None) and (early_stopper is not None):\n",
    "        if early_stopper.early_stop(training_history['val_loss'][-1]):\n",
    "            logger.debug(\n",
    "                f'Early stopping epoch: {epoch_counter}'\n",
    "                f' | Training loss: {training_history[\"training_loss\"][-1]}'\n",
    "                f' | Validation loss: {training_history[\"val_loss\"][-1]}'\n",
    "            )\n",
    "            \n",
    "            break\n",
    "    elif (early_stopper is not None):\n",
    "        if early_stopper.early_stop(training_history['training_loss'][-1]):\n",
    "            logger.debug(\n",
    "                f'Early stopping epoch: {epoch_counter}'\n",
    "                f' | Training loss: {training_history[\"training_loss\"][-1]}'\n",
    "            )\n",
    "            \n",
    "            break\n",
    "\n",
    "training_history['training_loss'] = torch.tensor(training_history['training_loss']).tolist()\n",
    "training_history['training_accuracy'] = torch.tensor(training_history['training_accuracy']).tolist()\n",
    "training_history['val_loss'] = torch.tensor(training_history['val_loss']).tolist()\n",
    "training_history['val_accuracy'] = torch.tensor(training_history['val_accuracy']).tolist()\n",
    "\n",
    "logger.info(f'Last epoch: {epoch_counter}')\n",
    "\n",
    "baseline_accuracy = 1. / q\n",
    "\n",
    "plot_training_history(training_history, baseline_accuracy=baseline_accuracy)\n",
    "\n",
    "logger.info(f'Final test accuracy: {training_history[\"val_accuracy\"][-1]} (baseline: {baseline_accuracy})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c352a195-6b6c-4aec-b49b-3f9dfad91937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_experiment_info(\n",
    "#     '../data/experiment_catalog.csv',\n",
    "#     **{\n",
    "#         'experiment_id': 'exp_s39',\n",
    "#         'matrix_type': matrix_type,\n",
    "#         'eps': eps,\n",
    "#         'sigma': sigma,\n",
    "#         'matrix_entropy': rho_entropy,\n",
    "#         'k': k,\n",
    "#         'q': q,\n",
    "#         'n_samples': n_samples,\n",
    "#         'dims': dims,\n",
    "#         'learning_rate': learning_rate,\n",
    "#         'batch_size': batch_size,\n",
    "#         'n_epochs': n_epochs,\n",
    "#         'final_train_loss': training_history['training_loss'][-1],\n",
    "#         'final_val_loss': training_history['val_loss'][-1],\n",
    "#         'final_train_accuracy': training_history['training_accuracy'][-1],\n",
    "#         'final_val_accuracy': training_history['val_accuracy'][-1],\n",
    "#         'baseline_accuracy': baseline_accuracy\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# experiment_catalog = load_experiment_catalog(EXPERIMENT_CATALOG_PATH)\n",
    "\n",
    "# experiment_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2d8090-8982-4111-b622-89e0e1085d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "matrix_type = 'simplified'\n",
    "\n",
    "sns.lineplot(\n",
    "    x=experiment_catalog[experiment_catalog['matrix_type'] == matrix_type].sort_values(by='matrix_entropy', ascending=True)['matrix_entropy'],\n",
    "    y=experiment_catalog[experiment_catalog['matrix_type'] == matrix_type].sort_values(by='matrix_entropy', ascending=True)['final_val_accuracy'],\n",
    "    label='Validation accuracy',\n",
    "    marker='o'\n",
    ")\n",
    "\n",
    "sns.lineplot(\n",
    "    x=experiment_catalog[experiment_catalog['matrix_type'] == matrix_type].sort_values(by='matrix_entropy', ascending=True)['matrix_entropy'],\n",
    "    y=experiment_catalog[experiment_catalog['matrix_type'] == matrix_type].sort_values(by='matrix_entropy', ascending=True)['final_train_accuracy'],\n",
    "    label='Training accuracy',\n",
    "    marker='o'\n",
    ")\n",
    "\n",
    "sns.lineplot(\n",
    "    x=experiment_catalog[experiment_catalog['matrix_type'] == matrix_type].sort_values(by='matrix_entropy', ascending=True)['matrix_entropy'],\n",
    "    y=experiment_catalog[experiment_catalog['matrix_type'] == matrix_type].sort_values(by='matrix_entropy', ascending=True)['baseline_accuracy'],\n",
    "    label='Baseline accuracy'\n",
    ")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Transition matrix entropy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy VS entropy\\n(simplified transition matrices)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e3c2ea-bafa-41f9-9aa2-1b9e1226e711",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "for matrix_type in experiment_catalog['matrix_type'].unique():\n",
    "    sns.lineplot(\n",
    "        x=experiment_catalog[experiment_catalog['matrix_type'] == matrix_type].sort_values(by='matrix_entropy', ascending=True)['matrix_entropy'],\n",
    "        y=experiment_catalog[experiment_catalog['matrix_type'] == matrix_type].sort_values(by='matrix_entropy', ascending=True)['final_val_accuracy'],\n",
    "        label=matrix_type,\n",
    "        marker='o'\n",
    "    )\n",
    "    \n",
    "plt.legend()\n",
    "plt.xlabel('Transition matrix entropy')\n",
    "plt.ylabel('Validation accuracy')\n",
    "plt.title('Accuracy VS entropy\\n(lognormal with $\\sigma = 5$)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e36cac-728c-490c-976b-f9eb2ae0d4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.lineplot(\n",
    "    data=experiment_catalog[\n",
    "        (experiment_catalog['matrix_type'] == 'simplified')\n",
    "        & (experiment_catalog['eps'] == 0.015)\n",
    "    ][['n_samples', 'final_val_accuracy']],#.drop_duplicates(),\n",
    "    x='n_samples',\n",
    "    y='final_val_accuracy'\n",
    ")\n",
    "\n",
    "plt.title('Final validation accuracy VS sample size\\n(simplified matrices, $\\epsilon$=0.015)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f29bfc2-a092-4a6e-b006-8f7fcd95b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_catalog['dims']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8cf802-a3df-452f-8d69-150965b5f768",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_catalog[\n",
    "        (experiment_catalog['matrix_type'] == 'simplified')\n",
    "        & (experiment_catalog['eps'] == 0.015)\n",
    "        & (experiment_catalog['n_samples'] == 8000)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b99242-99f3-464e-bde7-4ced916aa505",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_catalog['intermediate_layer_width'] = experiment_catalog['dims'].apply(lambda l: l[1] if len(l) == 3 else 0)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.lineplot(\n",
    "    data=experiment_catalog[\n",
    "        (experiment_catalog['matrix_type'] == 'simplified')\n",
    "        & (experiment_catalog['eps'] == 0.015)\n",
    "        & (experiment_catalog['n_samples'] == 8000)\n",
    "    ],\n",
    "    x='intermediate_layer_width',\n",
    "    y='final_val_accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8842e2e5-88df-4995-aa8c-11b43a51bd66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f02aae-8bfb-4862-b6a2-9e4104cebc58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5532a7c6-9115-4f7f-9f1f-a511027bfbe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443ccba4-13a6-4458-b090-bd3ce8efc4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa2d7c0-d8aa-4ccc-9a3c-b2f98cac74c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dab9c2b7-ff14-458a-8683-e7014acebb91",
   "metadata": {},
   "source": [
    "## Distribution of entropy of log-normally distributed transition matrices for different $\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3025012-32f5-4380-a975-e4823e0cdbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_generation import calcrho_lognormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5789d8-6ac4-4a07-985f-1605116706c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies_sigma = {}\n",
    "\n",
    "for sigma_test in range(1, 11):\n",
    "    logger.info(f'Sigma tested: {sigma_test}')\n",
    "    \n",
    "    entropies_sigma[sigma_test] = [compute_rho_entropy(calcrho_lognormal(q, sigma_test), q) for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2572e5-2af3-4df1-ae0d-8536bdbcdeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "for sigma_test in sorted(entropies_sigma.keys()):\n",
    "    sns.histplot(\n",
    "        entropies_sigma[sigma_test],\n",
    "        label=f'$\\sigma$={sigma_test}'\n",
    "    )\n",
    "\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
