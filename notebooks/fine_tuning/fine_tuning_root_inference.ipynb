{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "423912fd-7657-4519-a064-6fc77f59f5f6",
   "metadata": {},
   "source": [
    "# Fine-tuning for root inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c191253-a463-4119-b765-99bdd41142f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "sys.path.append('../../modules/')\n",
    "\n",
    "from logger_tree_language import get_logger\n",
    "from pytorch_utilities import load_checkpoint, count_model_params\n",
    "from models import replace_decoder_with_classification_head, freeze_encoder_weights\n",
    "from training import train_model\n",
    "from model_evaluation_tree_language import compute_accuracy\n",
    "from plotting import plot_training_history\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "logger = get_logger('fine_tuning_root_inference')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9393310-ae36-458f-af40-e017d3018fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINING_DATA_PATH = '../../data/mlm_data/slrm_data/labeled_data_fixed_4_8_1.0_0.00000.npy'\n",
    "DATA_PATH = '../../data/mlm_data/slrm_data/labeled_data_fixed_validation_4_8_1.0_0.00000.npy'\n",
    "MODEL_DIR = '../../models/mlm_pretraining_2/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e82cd3-7602-4a6c-a627-d2152be956d9",
   "metadata": {},
   "source": [
    "Load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1a5381-75ac-4aaa-a16f-273a5251500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "q, k, sigma, epsilon, roots_seeds, leaves_seeds, rho_seeds = np.load(DATA_PATH, allow_pickle=True)\n",
    "\n",
    "# The last index corresponds to the seed that generated the\n",
    "# data/transition tensors: select one.\n",
    "seed = 0\n",
    "\n",
    "shuffled_indices = np.random.choice(range(leaves_seeds.shape[1]), leaves_seeds.shape[1], replace=False)\n",
    "\n",
    "roots = roots_seeds[:, seed]\n",
    "roots = roots[shuffled_indices]\n",
    "\n",
    "leaves = leaves_seeds[..., seed].T\n",
    "leaves = leaves[shuffled_indices, :]\n",
    "rho = rho_seeds[..., seed]\n",
    "\n",
    "# Train-test split.\n",
    "n_samples_test = 2000\n",
    "\n",
    "leaves_train = leaves[:-n_samples_test, :]\n",
    "roots_train = roots[:-n_samples_test]\n",
    "\n",
    "leaves_test = leaves[-n_samples_test:, :]\n",
    "roots_test = roots[-n_samples_test:]\n",
    "\n",
    "logger.info(\n",
    "    f'N training samples: {leaves_train.shape[0]}'\n",
    "    f' | N test samples: {leaves_test.shape[0]}'\n",
    ")\n",
    "\n",
    "# Data preprocessing.\n",
    "leaves_train = torch.from_numpy(leaves_train).to(device=device).to(dtype=torch.int64)\n",
    "leaves_test = torch.from_numpy(leaves_test).to(device=device).to(dtype=torch.int64)\n",
    "\n",
    "# roots_train = torch.from_numpy(roots_train).to(device=device).to(dtype=torch.int64)\n",
    "# roots_test = torch.from_numpy(roots_test).to(device=device).to(dtype=torch.int64)\n",
    "roots_train = torch.nn.functional.one_hot(\n",
    "    torch.from_numpy(roots_train).to(dtype=torch.int64), num_classes=q\n",
    ").to(dtype=torch.float32).to(device=device)\n",
    "roots_test = torch.nn.functional.one_hot(\n",
    "    torch.from_numpy(roots_test).to(dtype=torch.int64), num_classes=q\n",
    ").to(dtype=torch.float32).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c959a807-af71-4854-912a-01b3666cc9dc",
   "metadata": {},
   "source": [
    "Load pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fe204e-0100-4df2-b310-de7a04fa0d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_epochs = sorted([\n",
    "    int(f.split('_')[-1].split('.')[0])\n",
    "    for f in os.listdir(MODEL_DIR)\n",
    "    if '.pt' in f\n",
    "])\n",
    "\n",
    "selected_checkpoint_epoch = checkpoint_epochs[-1]\n",
    "\n",
    "checkpoint_id = [f for f in os.listdir(MODEL_DIR) if f'{selected_checkpoint_epoch}.pt' in f][0]\n",
    "\n",
    "logger.info(f'Selected checkpoint: {checkpoint_id}')\n",
    "\n",
    "pretrained_model, _, training_history = load_checkpoint(\n",
    "    MODEL_DIR,\n",
    "    checkpoint_id,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "plot_training_history(training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452e55cc-2d06-438e-ba64-a9c1f35d8999",
   "metadata": {},
   "source": [
    "Replace the pretrained model's head with a new classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8385be1-f55e-41f7-a892-d5804092aac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = replace_decoder_with_classification_head(\n",
    "    pretrained_model,\n",
    "    n_classes=q,\n",
    "    device=device,\n",
    "    embedding_agg='flatten',\n",
    "    head_hidden_dim=[128, 64, 32],\n",
    "    head_activation='relu',\n",
    "    head_output_activation='identity',\n",
    "    head_batch_normalization=False,\n",
    "    head_dropout_p=None\n",
    ")\n",
    "\n",
    "del pretrained_model\n",
    "\n",
    "freeze_encoder_weights(classification_model, trainable_modules=['decoder'])\n",
    "\n",
    "logger.info(f'Total number of parameters: {count_model_params(classification_model)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6f9ac6-6bd8-4d40-88bb-45f896120bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4554a7-1a2e-4f93-b901-7afb19f603bf",
   "metadata": {},
   "source": [
    "Checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f62805f-1ade-4035-8425-229baef2741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Shape: (batch_size, q).\n",
    "pred = classification_model(leaves_train[:batch_size]).detach()\n",
    "\n",
    "print(pred.shape)\n",
    "\n",
    "compute_accuracy(pred, roots_train[:batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21962a8-83ab-444e-9dae-c3adff3ebfc0",
   "metadata": {},
   "source": [
    "Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa588ad-0125-4dc3-be53-ce2c52993e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_training_history = {\n",
    "    'training_loss': [],\n",
    "    'val_loss': [],\n",
    "    'training_accuracy': [],\n",
    "    'val_accuracy': [],\n",
    "    'learning_rate': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4560d2d0-ef07-41a3-9a0b-f955432475a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 25\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "_, fine_tuning_training_history = train_model(\n",
    "    model=classification_model,\n",
    "    training_data=(leaves_train, roots_train),\n",
    "    test_data=(leaves_test, roots_test),\n",
    "    n_epochs=n_epochs,\n",
    "    loss_fn=loss_fn,\n",
    "    learning_rate=1e-4,\n",
    "    batch_size=32,\n",
    "    early_stopper=None,\n",
    "    training_history=fine_tuning_training_history,\n",
    "    tensorboard_log_dir=None\n",
    ")\n",
    "\n",
    "plot_training_history(fine_tuning_training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f0a7cf-6a5b-4d05-aa07-e420b011a790",
   "metadata": {},
   "source": [
    "Hypotheses:\n",
    "- Pre-training with MLM does not help in our case.\n",
    "- We got some of the hyperparameters wrong.\n",
    "- Use a `<cls>` token?\n",
    "- Wrong data regime --> Not enough training data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
