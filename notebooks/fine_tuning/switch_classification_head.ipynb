{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "619b37c2-bed2-409a-937c-8ce9f0588064",
   "metadata": {},
   "source": [
    "# Switch the head of a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162599e4-cd0d-4848-95a9-a1dcfee4d289",
   "metadata": {},
   "source": [
    "__Objective:__ load a model pretrained on masked language modeling and replace its decoder head with a newly-initialized classification head, with the weights of all but the new head being kept frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "01fec282-cce3-47f1-a2a7-62c3e8ca7e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "\n",
    "sys.path.append('../../modules/')\n",
    "\n",
    "from pytorch_utilities import load_checkpoint\n",
    "from plotting import plot_training_history\n",
    "from models import FFNN\n",
    "from training import train_model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0063a8bf-868e-47f4-a92e-a14f6cd67c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = '../../models/mlm_pretraining_1/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dbb198-801c-4839-bc0c-f90ce6e71556",
   "metadata": {},
   "source": [
    "Select the model checkpoint to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "df7da731-2f42-467a-8dfe-81bb37873723",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_epochs = sorted([\n",
    "    int(f.split('_')[-1].split('.')[0])\n",
    "    for f in os.listdir(MODEL_DIR)\n",
    "    if '.pt' in f\n",
    "])\n",
    "\n",
    "selected_checkpoint_epoch = checkpoint_epochs[-1]\n",
    "\n",
    "checkpoint_id = [f for f in os.listdir(MODEL_DIR) if f'{selected_checkpoint_epoch}.pt' in f][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8092aba0-31c0-42db-af51-9cdac4bb953c",
   "metadata": {},
   "source": [
    "Load model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b54bdb63-a8d9-46f7-a7e6-5c4bdc1b2a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moscato/miniconda3/envs/tree-language/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model, optimizer, training_history = load_checkpoint(\n",
    "    MODEL_DIR,\n",
    "    checkpoint_id,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# plot_training_history(training_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "013e2725-d4d1-4537-8182-4a7778624444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256, 4])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick test on the output shape (assuming q=4 and l=8).\n",
    "test_batch = torch.randint(0, 4, (32, 256)).to(device=device)\n",
    "\n",
    "model(test_batch).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f42e723-a6ed-45f2-97dc-fc333bf71f32",
   "metadata": {},
   "source": [
    "Structure of the model (a `TranformerClassifier`, basically an encoder-only transformer with a fully-connected decoder), sequentially (with each item corresponding to an attribute):\n",
    "- `input_embedding` (a trainable `Embedding` layer).\n",
    "- `positional_embedding` (a `PositionalEncoding`, if positional encoding is used, otherwise the identity operation).\n",
    "- `transformer_encoder` (a stack of `TransformerEncoderLayer`s).\n",
    "- `embedding_agg_layer` (an operation on the latent representations provided by the encoder).\n",
    "- `decoder` (a fully-connected network mapping the chosen aggregation of the tokens' latent representations into the output)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25adafd-5371-428f-8716-ccf028b9a5b4",
   "metadata": {},
   "source": [
    "**Note:** there seems to be an overcounting of parameters between the **single** `TransformerEncoderLayer` instantiated and then passed to the constructor of the `TransformerEncoder` object, i.e. the parameter in the former are counted as parameters of the model even though actually only the copies of it used in the latter are used by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "94439188-0726-4d87-981b-c4043ae42dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_embedding 640\n",
      "positional_embedding 0\n",
      "encoder_layer 132480\n",
      "transformer_encoder 264960\n",
      "embedding_agg_layer 0\n",
      "decoder 8516\n",
      "\n",
      "Total n parameters (with overcounting of `TransformerEncoderLayer`): 406596\n",
      "\n",
      "Total n parameters (without overcounting of `TransformerEncoderLayer`): 274116\n"
     ]
    }
   ],
   "source": [
    "submodules = ['input_embedding', 'positional_embedding', 'encoder_layer', 'transformer_encoder', 'embedding_agg_layer', 'decoder']\n",
    "\n",
    "for submodule in submodules:\n",
    "    n_params_submodule = sum([p.numel() for p in getattr(model, submodule).parameters()])\n",
    "    \n",
    "    print(submodule, n_params_submodule)\n",
    "\n",
    "print('\\nTotal n parameters (with overcounting of `TransformerEncoderLayer`):', sum([p.numel() for p in model.parameters()]))\n",
    "print(\n",
    "    '\\nTotal n parameters (without overcounting of `TransformerEncoderLayer`):',\n",
    "    sum([p.numel() for p in model.parameters()]) - sum([p.numel() for p in model.encoder_layer.parameters()])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fce3a0-9e3d-4a3c-a3b1-676e9ebf2283",
   "metadata": {},
   "source": [
    "Replace the aggregation operation and the decoder with new ones and freeze the weights of the encoder part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "897a13fb-9cc9-44fa-89a1-676212886020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_decoder_with_classification_head(\n",
    "        original_model,\n",
    "        n_classes,\n",
    "        device,\n",
    "        head_hidden_dim=[64],\n",
    "        head_activation='relu'\n",
    "    ):\n",
    "    model = deepcopy(original_model)\n",
    "    \n",
    "    # Replace the aggregation operation from `None` (no aggregation, as\n",
    "    # needed for MLM) to `flatten`.\n",
    "    model.embedding_agg = 'flatten'\n",
    "    model.embedding_agg_layer = torch.nn.Flatten(start_dim=-2, end_dim=-1)\n",
    "    \n",
    "    decoder_input_dim = model.seq_len * model.embedding_size\n",
    "    \n",
    "    # Replace the decoder with a FFNN of appropriate size.\n",
    "    model.decoder = FFNN(\n",
    "        dims=(\n",
    "            [decoder_input_dim]\n",
    "            + head_hidden_dim\n",
    "            + [n_classes]\n",
    "        ),\n",
    "        activation=head_activation,\n",
    "        output_activation='softmax',\n",
    "        batch_normalization=False,\n",
    "        concatenate_last_dim=False\n",
    "    ).to(device=device)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def freeze_encoder_weights(model, trainable_modules=['decoder']):\n",
    "    \"\"\"\n",
    "    Freezes the weights in all the submodules of `models` whose name\n",
    "    doesn't appear in the `trainable_modules` list.\n",
    "    \"\"\"\n",
    "    for submodule in classification_model.named_children():\n",
    "        if submodule[0] not in trainable_modules:\n",
    "            for p in submodule[1].parameters():\n",
    "                p.requires_grad = False\n",
    "        \n",
    "        print(\n",
    "            submodule[0],\n",
    "            f'| N parameters: {sum([p.numel() for p in submodule[1].parameters()])}'\n",
    "            '| Parameters trainable:',\n",
    "            all([p.requires_grad for p in submodule[1].parameters()])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "4ef2f779-a137-4a0a-b693-861746f81193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_embedding | N parameters: 640| Parameters trainable: False\n",
      "positional_embedding | N parameters: 0| Parameters trainable: True\n",
      "encoder_layer | N parameters: 132480| Parameters trainable: False\n",
      "transformer_encoder | N parameters: 264960| Parameters trainable: False\n",
      "embedding_agg_layer | N parameters: 0| Parameters trainable: True\n",
      "decoder | N parameters: 2097476| Parameters trainable: True\n"
     ]
    }
   ],
   "source": [
    "classification_model = replace_decoder_with_classification_head(\n",
    "    model,\n",
    "    n_classes=4,\n",
    "    device=device,\n",
    "    head_hidden_dim=[64],\n",
    "    head_activation='relu'\n",
    ")\n",
    "\n",
    "freeze_encoder_weights(classification_model, trainable_modules=['decoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "19acd5c6-643a-43e3-a6bc-4fffcc480834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 256, 4]), torch.Size([32, 4]))"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick test on output shape (after having altered\n",
    "# the model).\n",
    "model(test_batch).shape, classification_model(test_batch).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d09db5-9c1c-46b2-aec2-fa9de80b45e3",
   "metadata": {},
   "source": [
    "## Check on the frozen weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c563757f-03e0-4658-8d97-023e5afd12ef",
   "metadata": {},
   "source": [
    "Check that after some epochs of (fake) fine-tuning the values of the frozen weights in the fine-tuned model are still the same as in the original pretrained model.\n",
    "\n",
    "**Note:** this can be checked by running the following with the call to `freeze_encoder_weights` (above) commented out VS not commented out - in the former case, no weights kept frozen, which can be checked below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "8ee811a9-d836-4aad-8a10-b23a9bb76fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batch = torch.randint(0, 4, (32, 256)).to(device=device)\n",
    "training_labels = torch.randint(0, 4, (32,)).to(device=device)\n",
    "\n",
    "test_batch = torch.randint(0, 4, (32, 256)).to(device=device)\n",
    "test_labels = torch.randint(0, 4, (32,)).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "65e2ec19-9dcb-494f-ba86-2127e68b068b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 13:52:30,519 - train_model - INFO - Training model\n",
      "100%|██████████████████████████| 10/10 [00:00<00:00, 101.84it/s, training_accuracy=tensor(0.), training_loss=tensor(1.0207), val_accuracy=tensor(0., device='cuda:0'), val_loss=tensor(1.3029, device='cuda:0')]\n",
      "2024-05-02 13:52:30,620 - train_model - INFO - Last epoch: 10\n"
     ]
    }
   ],
   "source": [
    "classification_model, fine_tuning_history = train_model(\n",
    "    classification_model,\n",
    "    training_data=(training_batch, training_labels),\n",
    "    test_data=(test_batch, test_labels),\n",
    "    n_epochs=10,\n",
    "    loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "    learning_rate=0.001,\n",
    "    batch_size=test_batch.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "5165ce0c-dbaf-4f43-9260-705dcd2b8dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_embedding True\n",
      "encoder_layer True\n",
      "transformer_encoder True\n"
     ]
    }
   ],
   "source": [
    "for submodule in zip(classification_model.named_children(), model.named_children()):\n",
    "    s_classif = submodule[0]\n",
    "    s_pretrained = submodule[1]\n",
    "    \n",
    "    n_params = sum([p.numel() for p in s_classif[1].parameters()])\n",
    "\n",
    "    if (n_params > 0) and (s_classif[0] != 'decoder'):\n",
    "        print(\n",
    "            s_classif[0],\n",
    "            all([(ps == pp).all() for ps, pp in zip(s_classif[1].parameters(), s_pretrained[1].parameters())])\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
