{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "842c967e-31b8-4023-b33b-c821425dd86a",
   "metadata": {},
   "source": [
    "# Root inference with padded sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38869a44-af70-42fa-ab28-189a1136fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "sys.path.append('../../modules/')\n",
    "\n",
    "from logger_tree_language import get_logger\n",
    "from tree_language_vocab import TreeLanguageVocab\n",
    "from tree_generation_nontrivial_topology import pad_sequence\n",
    "from models import TransformerClassifier\n",
    "from training import train_model\n",
    "from plotting import plot_training_history\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "logger = get_logger('root_inference_padding')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad7b09b-a06c-40ec-a65e-f6657dba130e",
   "metadata": {},
   "source": [
    "Load and preprocess data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857c532d-84e6-427e-a104-e3472ccc1d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../../data/topological_tree_data/topological_tree_data_4_5_1.0_0.0.pkl'\n",
    "\n",
    "q, l, sigma, epsilon = DATA_PATH.split('.pkl')[0].split('_')[-4:]\n",
    "\n",
    "q = int(q)\n",
    "l = int(l)\n",
    "sigma = float(sigma)\n",
    "epsilon = float(epsilon)\n",
    "max_seq_len = 2 ** l\n",
    "\n",
    "logger.info(f'Loading data from: {DATA_PATH}')\n",
    "logger.info(f'q = {q} | l = {l} | sigma = {sigma} | epsilon = {epsilon}')\n",
    "\n",
    "with open(DATA_PATH, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5a64f1-042a-4d8e-bec6-556069efb51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = TreeLanguageVocab(\n",
    "    n_symbols=q,\n",
    "    use_pad_token=True,\n",
    "    use_mask_token=True\n",
    ")\n",
    "\n",
    "roots = np.array([t[1] for t in data])\n",
    "leaves = np.vstack([\n",
    "    pad_sequence(t[0], max_seq_len, vocab('<pad>'))\n",
    "    for t in data\n",
    "])\n",
    "\n",
    "leaves = torch.from_numpy(leaves).to(device=device).to(dtype=torch.int64)\n",
    "roots = torch.from_numpy(roots).to(device=device).to(dtype=torch.int64)\n",
    "\n",
    "shuffled_indices = np.random.choice(range(leaves.shape[0]), leaves.shape[0], replace=False)\n",
    "\n",
    "n_samples_test = int(0.2 * leaves.shape[0])\n",
    "\n",
    "leaves_train = leaves[:-n_samples_test, ...]\n",
    "leaves_test = leaves[-n_samples_test:, ...]\n",
    "\n",
    "# Define and preprocess targets (with one-hot encoding).\n",
    "roots_train = torch.nn.functional.one_hot(\n",
    "    roots[:-n_samples_test, ...], num_classes=q\n",
    ").to(dtype=torch.float32).to(device=device)\n",
    "roots_test = torch.nn.functional.one_hot(\n",
    "    roots[-n_samples_test:, ...], num_classes=q\n",
    ").to(dtype=torch.float32).to(device=device)\n",
    "\n",
    "leaves_train.shape, roots_train.shape, leaves_test.shape, roots_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39709e7-dda7-415d-aede-70bd342dc94a",
   "metadata": {},
   "source": [
    "Define model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9b4f4a-7d14-46a5-854e-b327b236be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Instantiate model.\n",
    "root_inference_model_params = dict(\n",
    "    seq_len=max_seq_len,\n",
    "    embedding_size=embedding_size,\n",
    "    n_tranformer_layers=2,  # Good: 4\n",
    "    n_heads=1,\n",
    "    vocab_size=vocab_size,\n",
    "    encoder_dim_feedforward=2 * embedding_size,\n",
    "    positional_encoding=True,\n",
    "    n_special_tokens=len(vocab.special_symbols),  # We assume the special tokens correspond to the last `n_special_tokens` indices.\n",
    "    embedding_agg='flatten',\n",
    "    decoder_hidden_sizes=[64],  # Good: [64]\n",
    "    decoder_activation='relu',  # Good: 'relu'\n",
    "    decoder_output_activation='identity'\n",
    ")\n",
    "\n",
    "root_inference_model = TransformerClassifier(\n",
    "    **root_inference_model_params\n",
    ").to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f26d216-5f5c-415d-a157-d4b422cab9e4",
   "metadata": {},
   "source": [
    "Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aba0630-7ef7-4f3b-85b8-f10e37ebedb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e24f4a-8f09-4f91-8995-6327462fe639",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, training_history = train_model(\n",
    "    model=root_inference_model,\n",
    "    training_data=(leaves_train, roots_train),\n",
    "    test_data=(leaves_test, roots_test),\n",
    "    n_epochs=n_epochs,\n",
    "    loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "    learning_rate=learning_rate,\n",
    "    batch_size=32,\n",
    "    early_stopper=None,\n",
    "    training_history=None,\n",
    "    checkpointing_period_epochs=None,\n",
    "    model_dir=None,\n",
    "    checkpoint_id=None,\n",
    "    tensorboard_log_dir=None,\n",
    "    padding_token=vocab('<pad>')\n",
    ")\n",
    "\n",
    "plot_training_history(training_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
