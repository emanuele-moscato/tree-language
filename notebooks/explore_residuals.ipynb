{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b927ccff-cc77-462a-8ced-7b6d1f59887a",
   "metadata": {},
   "source": [
    "# Explore residuals from encoder layers' residual connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4beedb9-1e34-4e6c-9865-87b1657523af",
   "metadata": {},
   "source": [
    "__Objective:__ take a trained `TransformerClassifier` model and, for an arbitrary transformer encoder layer in its encoder, get the residuals that pass through the residual connections and the output of the attention block, and compare the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b538ea54-d73c-4c0c-9919-537d1dcca69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append('../modules/')\n",
    "\n",
    "from utilities import read_data\n",
    "from pytorch_utilities import load_checkpoint\n",
    "from models import TransformerClassifier, get_encoder_layer_residuals\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641453d0-aeb6-4040-99fd-46dcb8fad630",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '../models/inverse_pretraining/classification_model_epoch_200.pt'\n",
    "DATA_PATH = '../data/inverse_pretraining/labeled_data_fixed_4_4_1.0_0.00000.npy'\n",
    "SEED = 0\n",
    "N_VAL_SAMPLES = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f16f24-d278-40a2-ade6-55ae7dc09922",
   "metadata": {},
   "source": [
    "Load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d034e252-cc4c-4e40-a33e-7b8eed4b2ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "q, k, sigma, epsilon, roots, leaves, rho = read_data(DATA_PATH, SEED)\n",
    "\n",
    "val_leaves = torch.from_numpy(leaves[-N_VAL_SAMPLES:]).to(device=device).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cad4ce4-bf86-41d5-865c-3f744f8e2421",
   "metadata": {},
   "source": [
    "Load model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202e1fec-53f3-4f81-ab09-7f370aa44856",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerClassifier(\n",
    "    seq_len=leaves.shape[-1],\n",
    "    embedding_size=128,\n",
    "    n_tranformer_layers=4,\n",
    "    n_heads=1,\n",
    "    vocab_size=q,\n",
    "    encoder_dim_feedforward=2048,\n",
    "    positional_encoding=True,\n",
    "    n_special_tokens=0,\n",
    "    embedding_agg='flatten',\n",
    "    decoder_hidden_sizes=[64],\n",
    "    decoder_activation='relu',\n",
    "    decoder_output_activation='identity',\n",
    ")\n",
    "\n",
    "torch_checkpoint = torch.load(MODEL_PATH)\n",
    "model.load_state_dict(torch_checkpoint['model_state_dict'])\n",
    "\n",
    "model = model.to(device=device)\n",
    "\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c31988-6a9b-4528-8f03-ac8b394a2035",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals, attention_output = get_encoder_layer_residuals(model=model, layer_number=0, leaves=val_leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf79d41-9fc9-481e-8c87-b6f50c3c52fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97486401-7b88-4d56-9f2f-385fe577f734",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "norm_ratios_data = []\n",
    "\n",
    "for i in range(len(model.transformer_encoder.layers)):\n",
    "    residuals, attention_output = get_encoder_layer_residuals(model=model, layer_number=i, leaves=val_leaves)\n",
    "\n",
    "    norm_ratios = (attention_output.norm(dim=-1) / residuals.norm(dim=-1)).cpu().numpy().ravel()\n",
    "\n",
    "    norm_ratios_statistics = {\n",
    "        'layer_number': i,\n",
    "        'mean': norm_ratios.mean(),\n",
    "        'std': norm_ratios.std(),\n",
    "        'median': np.median(norm_ratios),\n",
    "        'percentile_5': np.percentile(norm_ratios, 5.),\n",
    "        'percentile_95': np.percentile(norm_ratios, 95.)\n",
    "    }\n",
    "\n",
    "    norm_ratios_data.append(norm_ratios_statistics)\n",
    "    \n",
    "    sns.histplot(\n",
    "        norm_ratios,\n",
    "        stat='density',\n",
    "        alpha=0.6,\n",
    "        label=f'Encoder layer {i}',\n",
    "        bins=10\n",
    "    )\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Distribution of norm(attention output)/norm(residual) ratios', fontsize=14)\n",
    "\n",
    "norm_ratios_data = pd.DataFrame(norm_ratios_data)\n",
    "\n",
    "norm_ratios_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48ed0b8-ba91-42ea-ba2e-538338c7b28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old way of computing the relative importance of residuals vs\n",
    "# attention outputs - the above one is probably better!\n",
    "# component_ratios = residuals / attention_output\n",
    "\n",
    "# fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "# component_ratios_data = []\n",
    "\n",
    "# for i in range(len(model.transformer_encoder.layers)):\n",
    "#     residuals, attention_output = get_encoder_layer_residuals(model=model, layer_number=i, leaves=val_leaves)\n",
    "\n",
    "#     component_ratios = (residuals / attention_output).mean(dim=-1).cpu().numpy().ravel()\n",
    "\n",
    "#     component_ratios_statistics = {\n",
    "#         'layer_number': i,\n",
    "#         'mean': component_ratios.mean(),\n",
    "#         'std': component_ratios.std(),\n",
    "#         'median': np.median(component_ratios),\n",
    "#         'percentile_5': np.percentile(component_ratios, 5.),\n",
    "#         'percentile_95': np.percentile(component_ratios, 95.)\n",
    "#     }\n",
    "\n",
    "#     component_ratios_data.append(component_ratios_statistics)\n",
    "\n",
    "#     component_ratios_filtered = component_ratios[\n",
    "#         (component_ratios > component_ratios_statistics['percentile_5'])\n",
    "#         & (component_ratios < component_ratios_statistics['percentile_95'])\n",
    "#     ]\n",
    "    \n",
    "#     sns.histplot(\n",
    "#         component_ratios_filtered[:1000],\n",
    "#         stat='density',\n",
    "#         alpha=0.6,\n",
    "#         label=f'Encoder layer {i}'\n",
    "#     )\n",
    "\n",
    "#     plt.legend()\n",
    "#     plt.title('Distribution of average [residual]/[attention output] ratios\\n(averaged over components in the hidden dimension)', fontsize=14)\n",
    "\n",
    "# component_ratios_data = pd.DataFrame(component_ratios_data)\n",
    "\n",
    "# component_ratios_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
