{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fac171-6df0-42dc-bd35-ae959e8e8e39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('../modules/')\n",
    "\n",
    "from training import train_model\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed9b80e-89c4-4327-a013-914b8a3fdca5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Naive Bayes benchmark\n",
    "def get_NB_accuracy(x0s,xis_int,train_frac):\n",
    "    xi_train, xi_test, x0_train, x0_test = train_test_split(xis_int.T, x0s, test_size=1-train_frac, random_state=0)\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(xi_train, x0_train)\n",
    "    y_pred = gnb.predict(xi_test)\n",
    "    return np.sum((x0_test == y_pred))/len(x0_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736c6f4d-e07c-4af1-a050-970e917876c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data that will be used\n",
    "q = 4\n",
    "l = 4\n",
    "sigma = 1.0\n",
    "epsilon = 0.0\n",
    "seed = 31\n",
    "\n",
    "N_learn = 500\n",
    "\n",
    "[q,l,sigma,epsilon,x0s,xis,M_s] = np.load('../data/labeled_data_{}_{}_{}_{:.5f}.npy'.format(q,l,sigma,epsilon),allow_pickle=True)\n",
    "x0 = x0s[:,seed]\n",
    "xi = xis[:,:,seed]\n",
    "train_frac = N_learn/len(x0)\n",
    "phi_NB = get_NB_accuracy(x0,xi,train_frac)\n",
    "print('Naives Bayes accuracy: {:.2f}, trained on {} and tested on {} trees.'.format(phi_NB,int(train_frac*len(x0)),int((1-train_frac)*len(x0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7c950d-8822-4df9-860a-9d31fc18e7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cf8b51-d1b8-4a53-88c6-a20872d56fde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert data to be used in pytorch\n",
    "sequences = torch.from_numpy(xi[:,:N_learn].T).to(dtype=int).to(device=device)\n",
    "labels = torch.from_numpy(x0[:N_learn]).to(dtype=int).to(device=device)\n",
    "sequences_test = torch.from_numpy(xi[:,N_learn:].T).to(dtype=int).to(device=device)\n",
    "labels_test = torch.from_numpy(x0[N_learn:]).to(dtype=int).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6adb61-578e-4a8e-932c-95638544d286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_embeddings = q # number of possible values for each position\n",
    "embedding_size = 128 # some number to be played with\n",
    "embedding = nn.Embedding(num_embeddings, embedding_size).to(device=device)\n",
    "embedded_sequences = embedding(sequences)\n",
    "\n",
    "embedded_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56837da-3aad-4d77-9b96-df17a7821183",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c6d716-05ce-49c9-8426-b4955e636a6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_heads = 4\n",
    "\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_size, nhead=num_heads, dim_feedforward=2048, batch_first=True)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4).to(device=device)\n",
    "\n",
    "positional_encoder = PositionalEncoding(d_model=embedding_size).to(device=device)\n",
    "out = transformer_encoder(positional_encoder(embedded_sequences))\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9283842-12c7-4e4e-a263-0a662a758e3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, transformer_encoder, positional_encoder, final_layer, seq_len):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.transformer_encoder = transformer_encoder\n",
    "        self.final_layer = final_layer\n",
    "        self.pos_encoder = positional_encoder\n",
    "\n",
    "        # self.avgpooling = nn.AvgPool1d(\n",
    "        #     kernel_size=self.seq_len,\n",
    "        # )\n",
    "        self.flatten = nn.Flatten(start_dim=-2, end_dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        # x = torch.mean(x.detach(), dim=1)\n",
    "        # x = self.avgpooling(torch.permute(x.detach(), dims=(0, 2, 1))).squeeze()\n",
    "        x = self.flatten(x)\n",
    "        x = self.final_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 500\n",
    "losses = np.empty(num_epochs)\n",
    "acurracies = np.empty(num_epochs)\n",
    "\n",
    "# Define the final layer\n",
    "# final_layer = nn.Linear(embedding_size,q).to(device=device)  # For mean.\n",
    "final_layer = nn.Linear(embedded_sequences.shape[-2] * embedded_sequences.shape[-1],q).to(device=device)\n",
    "\n",
    "# Add positional encoding to the model\n",
    "\n",
    "# Create the model\n",
    "model = TransformerClassifier(transformer_encoder, positional_encoder, final_layer, embedded_sequences.shape[1]).to(device=device)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "\n",
    "train_model(\n",
    "    model=model,\n",
    "    training_data=(embedded_sequences, labels),\n",
    "    test_data=(embedding(sequences_test), labels_test),\n",
    "    n_epochs=500,\n",
    "    loss_fn=loss_fn,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=32,\n",
    "    early_stopper=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de0c695-2731-4081-9877-c3ac36a01824",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "#plt.xscale('log')\n",
    "plt.ylabel('X-entropy loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d458dd1-02f9-4522-b5f9-95e546dd6ede",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(acurracies)\n",
    "plt.plot(np.arange(num_epochs),phi_NB*np.ones(num_epochs),color='k',ls='--')\n",
    "#plt.xlim((1,num_epochs))\n",
    "#plt.xscale('log')\n",
    "plt.ylabel('train accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb0cbf5-0b71-4de4-8d19-80bb199af9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy on the rest of dataset\n",
    "embedded_sequences_test = embedding(sequences_test)\n",
    "out_test = model(embedded_sequences_test)\n",
    "\n",
    "out_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ef463f-ea33-4de3-9332-871fff3a858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probs_test = nn.Softmax(dim=-1)(out_test)\n",
    "\n",
    "predicted_probs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041172b0-5647-4013-8114-35000a9e3be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = torch.argmax(predicted_probs_test, dim=-1)\n",
    "\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed45d15-4322-42f7-953f-8b95fe1dac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_test = (predicted_labels == labels_test).sum().item()/len(labels_test)\n",
    "\n",
    "print('Accuracy on the rest of the dataset: {:.2f}'.format(accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d4de12-9fa2-4b63-97cc-8bf7812190fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb30817a-219b-45d9-93c5-90000e2a25a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5620ea-7300-41e9-835d-5c9e39fe27f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data, param.data.shape, param.requires_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
